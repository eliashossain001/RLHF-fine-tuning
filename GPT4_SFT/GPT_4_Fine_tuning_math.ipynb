{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urwfhUqGJ2DX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load dataset into a Colab-compatible cache directory\n",
        "# ds = load_dataset(\"AI-MO/NuminaMath-CoT\", cache_dir=\"/content/hf_cache\")"
      ],
      "metadata": {
        "id": "NFl4uypDJ62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets huggingface_hub"
      ],
      "metadata": {
        "id": "glQOeDgmJ64v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset into a Colab-compatible cache directory\n",
        "ds = load_dataset(\"meta-math/MetaMathQA\", cache_dir=\"/content/hf_cache\")\n"
      ],
      "metadata": {
        "id": "HJQb1FePJ66q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)"
      ],
      "metadata": {
        "id": "rdic-nezJ68-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Split into train and validation"
      ],
      "metadata": {
        "id": "c7EIJeOtNizU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Shuffle and downsample to 3,000 examples\n",
        "sampled_ds = ds[\"train\"].shuffle(seed=42).select(range(3000))\n",
        "\n",
        "# Step 2: Split into 90% train, 10% validation\n",
        "split_ds = sampled_ds.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "train_data = split_ds[\"train\"]\n",
        "val_data = split_ds[\"test\"]\n"
      ],
      "metadata": {
        "id": "UxxPp55MNHEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert dataset to JSON"
      ],
      "metadata": {
        "id": "ecmRTJvKMxg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def save_openai_format(dataset_split, output_file):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a math tutor. Solve the user's problem step-by-step.\"\n",
        "    }\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for example in dataset_split:\n",
        "            user_prompt = example[\"original_question\"]\n",
        "            assistant_answer = example[\"response\"]\n",
        "\n",
        "            formatted = {\n",
        "                \"messages\": [\n",
        "                    system_message,\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                    {\"role\": \"assistant\", \"content\": assistant_answer}\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            f.write(json.dumps(formatted) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Saved to {output_file}\")\n",
        "\n",
        "# Step 4: Save to JSONL\n",
        "save_openai_format(train_data, \"training_data_math.jsonl\")\n",
        "save_openai_format(val_data, \"validation_data_math.jsonl\")\n"
      ],
      "metadata": {
        "id": "ll755TdiM1fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup API and Training File"
      ],
      "metadata": {
        "id": "cn3F8viLOIGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_key = \"PUT KEY\""
      ],
      "metadata": {
        "id": "oZsLC1cJN937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from time import sleep\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key = open_ai_key)"
      ],
      "metadata": {
        "id": "ICyMQ-QqN96G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_training_file(file_path):\n",
        "    \"\"\"Upload training file to OpenAI\"\"\"\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        response = client.files.create(\n",
        "            file=file,\n",
        "            purpose=\"fine-tune\"\n",
        "        )\n",
        "        return response.id"
      ],
      "metadata": {
        "id": "3Z5OvaNMN98p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the files\n",
        "training_file_id = upload_training_file(\"training_data_math.jsonl\")\n",
        "validation_file_id = upload_training_file(\"validation_data_math.jsonl\")\n",
        "\n",
        "print(\"Training File ID:\", training_file_id)\n",
        "print(\"Validation File ID:\", validation_file_id)"
      ],
      "metadata": {
        "id": "OgYIwzhVN9-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Fine-Tuning Job"
      ],
      "metadata": {
        "id": "FmHTb5vbOj3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_fine_tuning_job(training_file_id, validation_file_id=None, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    response = client.fine_tuning.jobs.create(\n",
        "        training_file=training_file_id,\n",
        "        validation_file=validation_file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response.id\n",
        "\n",
        "job_id = create_fine_tuning_job(training_file_id, validation_file_id)\n",
        "print(\"Fine-tuning Job ID:\", job_id)\n"
      ],
      "metadata": {
        "id": "iv2_CunTN-BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitor the Job"
      ],
      "metadata": {
        "id": "O4q-N8lOOr_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def monitor_job(job_id):\n",
        "\n",
        "    while True:\n",
        "        job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "        print(f\"Status: {job.status}\")\n",
        "\n",
        "        if job.status in [\"succeeded\", \"failed\"]:\n",
        "            return job\n",
        "\n",
        "        events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=5)\n",
        "        for event in events.data:\n",
        "\n",
        "            print(f\"Event: {event.message}\")\n",
        "        time.sleep(30)\n",
        "\n",
        "job = monitor_job(job_id)\n",
        "\n",
        "if job.status == \"succeeded\":\n",
        "    fine_tuned_model = job.fine_tuned_model\n",
        "    print(f\"üéØ Fine-tuned model ID: {fine_tuned_model}\")\n",
        "else:\n",
        "    print(\"‚ùå Fine-tuning failed.\")\n"
      ],
      "metadata": {
        "id": "zXUT9iYkN-DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job = client.fine_tuning.jobs.retrieve(\"ftjob-FyNj5dZG0QMeuftwb8Izwya0\")\n",
        "print(\"üìå Job Status:\", job.status)\n",
        "print(\"üì¶ Model ID:\", job.fine_tuned_model)\n"
      ],
      "metadata": {
        "id": "KERSqWWX3m9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-TLd72r6lcI0vF3UtolgH3qN6\", limit=10)\n",
        "for event in events.data:\n",
        "    print(f\"[{event.created_at}] {event.message}\")"
      ],
      "metadata": {
        "id": "0my4y-z35GtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5nD5ZPfm5Gvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUP6iwuv5Gx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zahsos_5Gzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Model"
      ],
      "metadata": {
        "id": "GI81VVU3O23-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model_id, question):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_id,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "eUR5YO9V4QAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = test_model(fine_tuned_model, \"If f(x) = 2x + 3, what is f(4)?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "eGpBZmV5O4O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integration test\n",
        "result2 = test_model(fine_tuned_model, \"Evaluate the definite integral of the function f(x) = 3x^2 + 2x + 1 from x = 0 to x = 2. Show your steps clearly.\")\n",
        "print(\"Integration result:\", result2)"
      ],
      "metadata": {
        "id": "oNWZskIe4cKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qb97Hz5t4cM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Phase"
      ],
      "metadata": {
        "id": "rQ1liGAmPJxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score matplotlib"
      ],
      "metadata": {
        "id": "LgaMQXtcPLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "upQtRcRhPMgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from openai import OpenAI\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "def evaluate_model(model_id, test_data, max_samples=100):\n",
        "    bleu_scores, rouge1_scores, rougeL_scores = [], [], []\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    for i, item in enumerate(test_data.select(range(max_samples))):\n",
        "        question = item['original_question']\n",
        "        reference = item['response']\n",
        "\n",
        "        prediction = test_model(model_id, question)\n",
        "\n",
        "        # Tokenization\n",
        "        reference_tokens = word_tokenize(reference)\n",
        "        prediction_tokens = word_tokenize(prediction)\n",
        "\n",
        "        # BLEU\n",
        "        bleu = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=smoothie)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE\n",
        "        rouge = scorer.score(reference, prediction)\n",
        "        rouge1_scores.append(rouge['rouge1'].fmeasure)\n",
        "        rougeL_scores.append(rouge['rougeL'].fmeasure)\n",
        "\n",
        "        print(f\"\\nExample {i+1}\")\n",
        "        print(\"Q:\", question)\n",
        "        print(\"Expected:\", reference[:200])\n",
        "        print(\"Predicted:\", prediction[:200])\n",
        "        print(f\"BLEU: {bleu:.4f}, ROUGE-1: {rouge['rouge1'].fmeasure:.4f}, ROUGE-L: {rouge['rougeL'].fmeasure:.4f}\")\n",
        "\n",
        "    return bleu_scores, rouge1_scores, rougeL_scores\n"
      ],
      "metadata": {
        "id": "hnILqcNpPPzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Step 1: Shuffle and downsample to 3,000 examples\n",
        "sampled_ds = ds[\"train\"].shuffle(seed=42).select(range(3000))\n",
        "\n",
        "# Step 2: Split into 90% train, 10% validation\n",
        "split_ds = sampled_ds.train_test_split(test_size=0.1, seed=42)\n",
        "test_data = split_ds[\"test\"]\n",
        "\n",
        "# Run evaluation\n",
        "bleu_scores, rouge1_scores, rougeL_scores = evaluate_model(fine_tuned_model, test_data, max_samples=100)\n"
      ],
      "metadata": {
        "id": "XMq6VYtPPP2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(scores, title):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(scores, bins=20, alpha=0.7)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Score\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(bleu_scores, \"BLEU Score Distribution\")\n",
        "plot_metric(rouge1_scores, \"ROUGE-1 Score Distribution\")\n",
        "plot_metric(rougeL_scores, \"ROUGE-L Score Distribution\")\n"
      ],
      "metadata": {
        "id": "W-GkdcMfPl4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}